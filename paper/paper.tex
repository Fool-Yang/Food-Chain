\documentclass[a4paper]{article}

% Packages
\usepackage{geometry}
\geometry{left=1.5cm, right=1.5cm, top=2.54cm, bottom=2.54cm}
\usepackage{graphicx, hyperref, setspace, amsmath, amssymb, titlesec, multicol, parskip, indentfirst, etoolbox, caption, xcolor, svg}
\usepackage[style=ieee]{biblatex}
% Import the bibliography file
\addbibresource{ref.bib}

% Title Formatting
\titleformat{\section}{\centering\large\scshape}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{1em}{}
\setstretch{1.0} % Keep single spacing
\setlength{\parskip}{6pt} % Space between paragraphs
\titlespacing{\section}{0pt}{6pt}{6pt}
\titlespacing{\subsection}{0pt}{6pt}{6pt}
\titlespacing{\subsubsection}{0pt}{6pt}{6pt}
% Document Title
\title{
    \textbf{Draft v1}
}

\date{}
\captionsetup{labelfont={small,sc}, textfont={small,sc}}
% Section Numbering
% Define numbering format
\renewcommand{\thesection}{\Roman{section}.}
\renewcommand{\thesubsection}{\textit{\Alph{subsection}.}}
\renewcommand{\thesubsubsection}{\textit{\arabic{subsubsection}.}}
\renewcommand{\thetable}{\Roman{table}} % Set table numbering to Roman
\renewcommand{\thefigure}{\Roman{figure}} % Number figures in Roman numerals

% Make subsection titles italic as well
\titleformat{\subsection}{\normalfont\large\itshape}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\itshape}{\thesubsubsection}{1em}{}

\begin{document}

\maketitle
\vspace{-1.5cm}

% Authors Block
\begin{multicols}{3}
    \centering
    \textbf{Yang Li}\\
    \textit{University of the West of England}\\
    \textit{yang4.li@uwe.ac.uk}\\
    \vfill

    \textbf{Marco Perez Hernandez}\\
    \textit{University of the West of England}\\
    \textit{marco.perezhernandez@uwe.ac.uk}\\
    \vfill

    \textbf{Mehmet Aydin}\\
    \textit{University of the West of England}\\
    \textit{mehmet.aydin@uwe.ac.uk}\\
    \vfill
\end{multicols}

\singlespacing
\setlength{\parskip}{6pt}
\setlength{\parindent}{0.5cm}

\begin{abstract}
The abstract should be written after the rest of the paper is completed.
\end{abstract}

\begin{multicols}{2}
\setlength{\columnsep}{0.5cm}

\section{Introduction}
Many real-world problems can be modeled as non-cooperative games, such as business competition, political campaigns, stock trading, bidding in auctions, cyber security, and traffic routing. These problems feature the competitive nature of games because they share one thing in common, parties with conflicting interests. Plenty of research has been done on multi-agent reinforcement learning (MARL) in fully cooperative games. There is currently a gap in the non-cooperative setting \cite{zhu2024survey}. Most current SOTA MARL algorithms are designed for cooperative games.

In fully cooperative games, there is no conflict between agents. If agents can communicate, they are motivated to form a firm alliance that essentially works as a collective mind that is the one and only conscious being. Thus, in a game theory sense, a fully cooperative game where agents share information is equivalent to a single agent game where the collective agent tries to find the optimal joint action, given the joint observation. To do that in a traditional single agent setting where the environment is stationary and non-adversarial, one only needs to learn the environment mechanics (the state transition function) and the reward function so that the Q-values of an action in a given state can be computed or predicted by a function approximator. A Pareto optimal pure (deterministic) strategy is guaranteed to exist in fully cooperative finite games, meaning that the agents can always deterministically choose the optimal action that has the highest payoff. The only thing that keeps the agents from forming a cluster in practice is the hardware incapability (computational power, real-world constraints, etc.).

Non-cooperative environments, however, involve conflicts that make learning more complicated. Agents may belong to different parties that would like to have private control to protect their own interests. For example, the stock market is often considered a 0-sum fully competitive game where one person's profit is another person's loss, if we omit the long-term economic growth.

For cooperative games, we can always say that 5 rewards is better than 2 rewards, but what about the rewards $\langle 1,5 \rangle$ and  $\langle 10,1 \rangle$ where each agent gets one value in the vector? Rational self-interested agents will have to fight for what is best for them. Many solutions in game theory are possible. One of the commonly accepted solutions for is the Nash equilibrium (NE). NEs are normally what we want the agents to converge to because convergence means stability and strict NEs are the only stable points in the game. Generally, there may only be a mixed NE, meaning that players adopt randomized strategies consisting of probability distributions over the action space.

Currently, most SOTA MARL algorithms cannot learn a mixed policy. Largely due to the studied games are mostly fully cooperative or asynchronous. As we will show in the experiments, those algorithms will fail to find the optimal action in games specifically designed to challenge the agents' competitive thinking. We need to reconsider our approach if we want to build agents that are capable of dealing with non-fully-cooperative environments.

\subsection{Aim}
The aim of this study is to point out that current SOTA algorithms can be misleadingly performing well in non-fully-cooperative environments by both formal analysis and showing specific game scenarios that are designed to challenge the agents from a game theory perspective.

\subsection{Stochastic Game}

A stochastic game is formally defined by a tuple:

$G = \langle I,S,S_0,S_t,\{A_1,\ldots,A_n\},\{R_1,\ldots,R_n\},T \rangle$
\begin{itemize}
    \item $I$: the set of agents, from 1 to \(n\).
    \item $S$: the gameâ€™s state space.
    \item $S_0$: the set of possible initial states.
    \item $S_t$: the set of terminal states.
    \item $\{A_1,\ldots,A_n\}$: one action space for each agent.

    Let $a_i \in A_i$ be an action of agent $i$, and $a^n=\langle a_1,\ldots,a_n \rangle \in A^n$ denote a joint action in the joint action space.
    \item $\{R_1,\ldots,R_n\}$: one reward function for each agent.

    $R_i:S \times A^n \times S \rightarrow \mathbb{R}$
    \item $T$: the state transition function.

    $T:S \times A^n \times S \rightarrow [0,1]$

    It defines the probability of the game transitions from one state to another given the joint action. It must satisfy the axiom of probability:
    \begin{equation}
    \forall s \in S,a^n \in A^n: \sum_{s' \in S}{T(s,a^n,s')}=1
    \label{eq:statetransition}
    \end{equation}
    It is convenient to define $T^*:S \times A^n \rightarrow \Pi_S$, where $\Pi_S$ denotes the set of all probability distribution over the set S.

    Some games incorporate partial observability and define an observations space, which is irrelevant to the purpose of this study.
\end{itemize}

 Starting from a randomly chosen $s_0 \in S_0$, at each timestep $t \in \mathbb{N}$ with the current state $s_t$, each agent observes the current state $s_t$ and chooses an action forming a joint action $a^n=\langle a_1,\ldots,a_n \rangle$. The game then moves into the next state $s_{t+1}$, randomly sampled from the distribution P and a reward $R_i$ is assigned to each agent. If $s_t \in S_t$, the game ends. Otherwise, the game continues in the next timestep.

The goal is to find the optimal strategy (policy) for an agent to pick actions given the current state information they receive from the environment, such that their expected cumulative reward is maximized.

To demonstrate the problem with competitive games, let's analyze some simple normal form games. A normal form game has only two states, the starting state and the end state. The game ends immediately after all agents pick an action. Thus, they can be represented using the reward matrix. Consider rock-paper-scissors (simple one-shot matrix game) in Table~\ref{table:rps} for example. Each entry represents the reward for the row player and the column player, respectively.

\noindent
\begin{minipage}{\columnwidth}
\captionof{table}{Rock Paper Scissors} 
\label{table:rps}
\centering
\begin{tabular}{c|ccc}
    & r & p & s\\
    \hline
    r & 0,0 & -1,1 & 1,-1 \\
    p & 1,-1 & 0,0 & -1,1 \\
    s & -1,1 & 1,-1 & 0,0 \\
\end{tabular}
\end{minipage}

The only NE in the game is that two players play each of the three actions with 1/3 chance. If any player deviates from the NE, their expected reward does not increase, and the other player can exploit them by playing the action that counters their most frequent action. Here is a more complicated game called biased rock paper scissors.

\noindent
\begin{minipage}{\columnwidth}
\captionof{table}{Biased Rock Paper Scissors} 
\label{table:biasedrps}
\centering
\begin{tabular}{c|ccc}
    & r & p & s\\
    \hline
    r & 50,50 & 25,75 & 100,0 \\
    p & 75,25 & 50,50 & 45,55 \\
    s & 0,100 & 55,45 & 50,50 \\
\end{tabular}
\end{minipage}

The only NE in the game is that both players adopt the following policy.
\begin{equation}
\pi=(P(r)=0.0625,P(p)=0.6250,P(s)=0.3125)
\label{eq:biasedrpspolicy}
\end{equation}

The RL algorithm needs to learn $\pi$, not the optimal action because there is no optimal action. For competitive play after the learning process, the agent should play randomly according to the learned distribution.

\section{MARL in Competitive Games}
Algorithms that are carried over from the traditional single agent RL domain, such as Q-learning, do not incorporate randomized policy, even though they may behave randomly during the learning process (using an $\epsilon$-greedy exploration vs. exploitation strategy, for example). In the single agent setting, the agent only needs to learn the expected long-term return (the Q-value) of each action and deterministically pick the highest. Thus, they cannot learn $\pi$.

There is a family of algorithms called Joint Action Learning (JAL) that addresses the problem by incorporating the Q-values for the joint action. Minimax Q-learning \cite{littman1994markov} is a JAL algorithm that can be applied to two-player zero-sum games. It computes local minimax solutions for each state based on the learned joint Q-values. Nash Q-learning \cite{hu2003nash} is a similar algorithm that computes the local NE instead. It converges to a NE under highly strict assumptions that is found in almost no games. Correlated Q-learning \cite{greenwald2003correlated} computes a correlated equilibrium and has no known convergence guarantees. \cite{zinkevich2005cyclic} showed that JAL cannot converge in certain types of ill-conditioned games.

Some other research \cite{qu2020distributed} tried to address this problem using a manually calculated distribution over the actions based on the payoff, such that the higher the payoff of an action, the more likely the action will be chosen. This approach intuitively makes sense. The problem is that in any NE, for any action that are chosen with a positive probability, the payoff of the actions are the same. If we calculate the expected reward of each action in the biased rock-paper-scissors example where the opponent uses the NE strategy, we will see all three actions have the same payoff.

Suppose the algorithm can converge to a NE, then all Q-values should be the same (ignoring worthless actions). Such an algorithm will output a uniform distribution. The algorithm is essentially learning a pure random policy. This contradicts with the assumption that they converge to a NE. We have shown that approach cannot work through a proof by contradiction.

Most MARL problems studied are much more than single state matrix games and have astronomically large state/action spaces. It is impossible for the agents to converge to a global NE in the first place because of the limited computational power. Because of the complexity of the environment that many studies carry out in, picking the action with the best Q-value still gives reasonably good results since at least the agents learned to interact with the environment. The competitiveness of the game is not going to play a huge role before the agents can really predict the state transitions. Currently, many fully competitive games are studied in the MARL community such as Pommerman \cite{resnick2018pommerman}, Go \cite{silver2017mastering}, StarCraft II \cite{vinyals2019grandmaster}, and Dota 2 \cite{berner2019dota}. There are also some general environments that are non-cooperative such as hide-and-seek \cite{baker2019emergent}, Neural MMO \cite{suarez2021neural}, PettingZoo \cite{terry2021pettingzoo}, and Melting Pot \cite{leibo2021scalable}. However, we lack a standard benchmark for MARL algorithms in non-cooperative games due to the difficulty of measuring agents' performance in general non-cooperative games.
FightLadder \cite{li2024fightladder} used the Elo rating system to test MARL algorithms but they were restricted to two-player zero-sum fighting games.

\section{Methods}
We define the mechanics of the game Food Chain. I also will have to migrate the tables and charts from my previous Word documents to explain the unique characteristics that make it challenging which are not found in other games (especially compare to the hide and seek game).

The game is complex enough to eliminate any tabular approaches. The state space is roughly 1060 for just a 9x9 board and can grow exponentially. Function approximation methods, such as deep neural networks or Monte Carlo methos must be used to solve the whole game. The spacial information on the game board can also be a good challenge to algorithms using CNNs. The game is still simple enough to be human playable. The mechanics of the game (the state transition function) is very straightforward and easy for the agents to learn, so that they can focus on agent interactions. Because of the large state space, it is impossible for us to check whether the agents converged to a global NE since it is NP-Hard. Yet it has carefully designed mechanics (see my Casual Monthly Progress Report for Nov 2024) which very often force the agents to adopt random policies (and addressing the coordination problem, too). These are the leaf nodes of the game tree, which is equivalent to a simple matrix game. We can actually check whether the agents converge to NEs in those sub-cases easily. So, in summary.
\begin{itemize}
    \item It is simple enough to let the agents focus on interactions instead of the environment.
    \item It is complex enough to be interesting.
    \item It forces the agents to constantly address the mixed strategy problem.
    \item We can analyze the performance of the agents with game theory.
\end{itemize}

\section{Results}
Figure \ref{fig:hopperreward}, \ref{fig:frogreward}, \ref{fig:snakereward} shows the average test return for each type of agent after training in a randomly generated game environment. Figure \ref{fig:scenhopperreward}, \ref{fig:scenfrogreward} demonstrates that in the carefully set up matching pennies scenario. The results show that the algorithms could not reach the optimal reward which can be computed using linear algebra (which is also another strong aspect of the game. we can analyze the game formally and see whether the agents reached optimality). More formal analysis coming...

\noindent
\begin{minipage}{\columnwidth}
\centering
\includesvg[width=1\textwidth]{plots/rewards_global_hopper.svg}
\captionof{figure}{hopper rewards}
\label{fig:hopperreward}
\end{minipage}

\noindent
\begin{minipage}{\columnwidth}
\centering
\includesvg[width=1\textwidth]{plots/rewards_global_frog.svg}
\captionof{figure}{frog rewards}
\label{fig:frogreward}
\end{minipage}

\noindent
\begin{minipage}{\columnwidth}
\centering
\includesvg[width=1\textwidth]{plots/rewards_global_snake.svg}
\captionof{figure}{snake rewards}
\label{fig:snakereward}
\end{minipage}

\noindent
\begin{minipage}{\columnwidth}
\centering
\includesvg[width=1\textwidth]{plots/rewards_scen_hopper.svg}
\captionof{figure}{snake rewards}
\label{fig:scenhopperreward}
\end{minipage}

\noindent
\begin{minipage}{\columnwidth}
\centering
\includesvg[width=1\textwidth]{plots/rewards_scen_frog.svg}
\captionof{figure}{snake rewards}
\label{fig:scenfrogreward}
\end{minipage}

\section{Discussion}

\section{Conclusion}

\section*{Funding}
This research was funded by the University of the West of England.

% Prints bibliography
\printbibliography

\end{multicols}
\end{document}